{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Логарифмическая функция связи | $$f(x, \\theta) = \\exp(\\sum\\limits_{i=1}^n\\theta_ix_i + \\theta_0)$$|\n",
    "|-|-|\n",
    "|Функция потерь|$$l(X, y, \\theta) = \\frac{1}{m}\\sum\\limits_{i=1}^n(y_i\\log\\frac{y_i}{f(X_i, \\theta)} - y_i + f(X_i, \\theta)) + \\frac{\\alpha}{2}\\sum\\limits_{i=1}^n\\theta_i^2$$|\n",
    "|D2 метрика|$$D^2 = 1 - \\frac{D(y, \\hat{y})}{D(y, \\overline{y})}$$|\n",
    "|где D это:|$$D(y, \\hat{y}) = 2(y\\log\\frac{y}{\\hat{y}} - y + \\hat{y})$$| \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial{l}}{\\partial\\theta_k} = \\frac{1}{m}{\\sum\\limits_{i=1}^m(X_{i,k}(e^{\\sum\\limits_{j=1}^n\\theta_jX_{i,j} + \\theta_0} - y_i))+\\alpha\\theta_k},k=1...n$$\n",
    "$$ \\frac{\\partial{l}}{\\partial\\theta_0} = \\frac{1}{m}{\\sum\\limits_{i=1}^m(e^{\\sum\\limits_{j=1}^n\\theta_jX_{i,j}+ \\theta_0} - y_i)},k=0$$\n",
    "\n",
    "$$X=\\begin{vmatrix}\n",
    "X_{1,0} & X_{1,1} & ... & X_{1,n}\\\\\n",
    "X_{2,0} & X_{2,1} & ... & X_{2,n}\\\\\n",
    "... & ... & ... & ...\\\\\n",
    "X_{m,0} & X_{1,1} & ... & X_{m,n}\\\\\n",
    "\\end{vmatrix};\n",
    "X_{1...m,0}=1;\n",
    "\\theta=\\begin{vmatrix}\n",
    "\\theta_0 & \\theta_1 & ... & \\theta_n\n",
    "\\end{vmatrix};\n",
    "y=\\begin{vmatrix}\n",
    "y_1 \\\\ y_2 \\\\ ... \\\\ y_m\n",
    "\\end{vmatrix};\n",
    "\\alpha=\\begin{vmatrix}\n",
    "\\alpha_0 \\\\ \\alpha_1 \\\\ ... \\\\ \\alpha_n\n",
    "\\end{vmatrix};\n",
    "\\alpha_0 = 0\n",
    "$$\t\n",
    "$$\\frac{\\partial{l}}{\\partial\\theta_k} = \\frac{1}{m}{(X(e^{\\theta{X}} - y_i))+{\\alpha}{\\theta}}$$\n",
    "\n",
    "\n",
    "$$m_t = {\\beta_1}{m_{t-1}} + (1-\\beta_1)grad_t$$\n",
    "$$v_t = {\\beta_2}{v_{t-1}} + (1-\\beta_2)grad_t^2$$\n",
    "$$\\hat{m_t}=\\frac{m_t}{1-\\beta_1^t}$$\n",
    "$$\\hat{v_t}=\\frac{v_t}{1-\\beta_2^t}$$\n",
    "$$\\theta_{t+1}=\\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v_t}}+\\epsilon}\\hat{m_t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "from scipy import special\n",
    "\n",
    "class PoissonRegression:\n",
    "    \n",
    "    def __init__(self, use_bias: bool = True, alpha: float = 1):\n",
    "        self.use_bias = use_bias\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return np.exp(X @ self._theta + self._bias)\n",
    "        \n",
    "    def loss(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        y_predict = self.predict(X)\n",
    "        m = len(y)\n",
    "        return 1/m * (special.xlogy(y, (y / y_predict)) - y + y_predict).sum() + self.alpha/2 * (np.power(self._theta, 2).sum())\n",
    "\n",
    "    def score(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        y_predict = self.predict(X)\n",
    "        m = len(y)\n",
    "        D = lambda y, y_hat: np.sum(special.xlogy(y, (y / y_hat)) - y + y_hat)\n",
    "        return (1 - D(y, y_predict)/D(y, y.mean()))\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, *,\n",
    "            eta: float = 1e-3, beta1: float = 0.9, beta2: float = 0.999,\n",
    "            epsilon: float = 1e-8, tol: float = 1e-3, max_iter: int = 1000) -> PoissonRegression:\n",
    "        \n",
    "        if self.use_bias:\n",
    "            X_new = np.append(np.ones(shape=(X.shape[0], 1)), X , axis=1)\n",
    "            cnt_example, cnt_feature = X_new.shape \n",
    "            self._coeffs    = np.zeros(cnt_feature)\n",
    "            m               = np.zeros(cnt_feature)\n",
    "            v               = np.zeros(cnt_feature)\n",
    "            \n",
    "            alpha_new = np.ones(cnt_feature)*self.alpha\n",
    "            alpha_new[0] = 0  \n",
    "            \n",
    "            \n",
    "            #Ищем оптимальные параметры ADAM'ом\n",
    "            for t in range(1, max_iter+1):\n",
    "                grad = find_gradient(X_new, y, cnt_example) #gradient\n",
    "                if tol <= np.linalg.norm(grad):  #L2-norm\n",
    "                    m = beta1 * m + (1 - beta1) * grad\n",
    "                    v = beta2 * v + (1 - beta2) * grad**2\n",
    "                    m_corr = m / (1 - beta1**t)\n",
    "                    v_corr = v / (1 - beta2**t)\n",
    "                    self._coeffs = self._coeffs - eta * m_corr / (np.sqrt(v_corr) + epsilon)\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            self._theta = self._coeffs[1:]\n",
    "            self._bias = self._coeffs[0]\n",
    "        \n",
    "        else:\n",
    "            X_new = np.copy(X)\n",
    "            cnt_example, cnt_feature = X_new.shape \n",
    "            self._coeffs    = np.zeros(cnt_feature)\n",
    "            m               = np.zeros(cnt_feature)\n",
    "            v               = np.zeros(cnt_feature)\n",
    "              \n",
    "            find_gradient = lambda X, y, m: 1/m * (np.exp(X @ self._coeffs) - y) @ X + self.alpha * self._coeffs\n",
    "            \n",
    "            #Ищем оптимальные параметры ADAM'ом\n",
    "            for t in range(1, max_iter+1):\n",
    "                grad = find_gradient(X_new, y, cnt_example) #gradient\n",
    "                if tol < np.linalg.norm(grad):  #L2-norm\n",
    "                    m = beta1 * m + (1 - beta1) * grad\n",
    "                    v = beta2 * v + (1 - beta2) * grad**2\n",
    "                    m_corr = m / (1 - beta1**t)\n",
    "                    v_corr = v / (1 - beta2**t)\n",
    "                    self._coeffs = self._coeffs - eta * m_corr / (np.sqrt(v_corr) + epsilon)\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            self._theta = self._coeffs\n",
    "            self._bias = 0\n",
    "         \n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def coeffs(self) -> np.ndarray:\n",
    "        return self._theta\n",
    "    \n",
    "    @property\n",
    "    def bias(self) -> float:\n",
    "        return self._bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[10,50],\n",
    "              [20,30],\n",
    "              [25,30],\n",
    "              [20,60],\n",
    "              [15,70],\n",
    "              [40,40],\n",
    "              [30,45],\n",
    "              [20,45],\n",
    "              [40,30],\n",
    "              [7,35]])\n",
    "y = np.array([1, 2, 2, 1, 1, 2, 2, 1, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0345942 , 1.56285014, 1.72699681, 1.13584479, 0.92415611,\n",
       "       2.09514788, 1.62692136, 1.3323495 , 2.33031232, 1.14299596])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = PoissonRegression(use_bias=True)\n",
    "model = clf.fit(x, y)\n",
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01997456, -0.01063782])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36615452261272596"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021267104373499095"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7526645030513504"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7526645030513504"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Methods_Machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
