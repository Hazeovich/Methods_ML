{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Oracle:\n",
    "    def value(self, x: np.ndarray) -> float:\n",
    "        pass\n",
    "        \n",
    "    def gradient(self, x: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "class Adam:\n",
    "\n",
    "    eta: float\n",
    "    beta1: float\n",
    "    beta2: float\n",
    "    epsilon: float\n",
    "\n",
    "    def __init__(self, *, eta: float = 0.1, beta1: float = 0.9, beta2: float = 0.999, epsilon: float = 1e-8):   \n",
    "        self.eta = eta\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def optimize(self, oracle: Oracle, x0: np.ndarray, *, max_iter: int = 100, eps: float = 1e-5) -> np.ndarray:\n",
    "        \n",
    "        self._coeffs = np.copy(x0)\n",
    "        m = np.zeros(len(x0))\n",
    "        v = np.zeros(len(x0))\n",
    "        \n",
    "        for t in range(1, max_iter+1):\n",
    "            grad = oracle.gradient(self._coeffs) #gradient\n",
    "            if eps <= np.linalg.norm(grad):  #L2-norm\n",
    "                m = self.beta1 * m + (1 - self.beta1) * grad\n",
    "                v = self.beta2 * v + (1 - self.beta2) * grad**2\n",
    "                m_corr = m / (1 - self.beta1**t)\n",
    "                v_corr = v / (1 - self.beta2**t)\n",
    "                self._coeffs = self._coeffs - self.eta * m_corr / (np.sqrt(v_corr) + self.epsilon)\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        return self._coeffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m f \u001b[39m=\u001b[39m Oracle()\n\u001b[0;32m     12\u001b[0m adam \u001b[39m=\u001b[39m Adam()\n\u001b[1;32m---> 13\u001b[0m adam\u001b[39m.\u001b[39;49moptimize(f, np\u001b[39m.\u001b[39;49mones(\u001b[39m3\u001b[39;49m)\u001b[39m*\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[1], line 32\u001b[0m, in \u001b[0;36mAdam.optimize\u001b[1;34m(self, oracle, x0, max_iter, eps)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, max_iter\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m     31\u001b[0m     grad \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39mgradient(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coeffs) \u001b[39m#gradient\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     \u001b[39mif\u001b[39;00m eps \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mnorm(grad):  \u001b[39m#L2-norm\u001b[39;00m\n\u001b[0;32m     33\u001b[0m         m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta1 \u001b[39m*\u001b[39m m \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta1) \u001b[39m*\u001b[39m grad\n\u001b[0;32m     34\u001b[0m         v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta2 \u001b[39m*\u001b[39m v \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta2) \u001b[39m*\u001b[39m grad\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\haze1\\Study_notebooks\\Methods_ML\\Methods_Machine_learning\\lib\\site-packages\\numpy\\linalg\\linalg.py:2526\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2524\u001b[0m     sqnorm \u001b[39m=\u001b[39m x_real\u001b[39m.\u001b[39mdot(x_real) \u001b[39m+\u001b[39m x_imag\u001b[39m.\u001b[39mdot(x_imag)\n\u001b[0;32m   2525\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2526\u001b[0m     sqnorm \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mdot(x)\n\u001b[0;32m   2527\u001b[0m ret \u001b[39m=\u001b[39m sqrt(sqnorm)\n\u001b[0;32m   2528\u001b[0m \u001b[39mif\u001b[39;00m keepdims:\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# class Oracle:\n",
    "#     '''Provides an interface for evaluating a function and its derivative at arbitrary point'''\n",
    "    \n",
    "#     def value(self, x: np.ndarray) -> float:\n",
    "#         return x.sum()\n",
    "        \n",
    "#     def gradient(self, x: np.ndarray) -> np.ndarray:\n",
    "#         return np.ones(len(x))\n",
    "f = Oracle()\n",
    "adam = Adam()\n",
    "adam.optimize(f, np.ones(3)*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Methods_Machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
