{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "class Activation:\n",
    "    \"\"\"Base activation class\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._input = None\n",
    "\n",
    "    @property\n",
    "    def input(self):\n",
    "        \"\"\"Returns the last input received by the activation\"\"\"\n",
    "        return self._input\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Computes activation output\n",
    "        \n",
    "        Arguments:\n",
    "            x: Input array of shape (`batch_size`, ...)\n",
    "\n",
    "        Returns:\n",
    "            An array of the same shape as `x`\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Computes loss gradient with respect to the activation input.\n",
    "        \n",
    "        Arguments:\n",
    "            gradOutput: Gradient of loss function with recpect to the activation output.\n",
    "                An array of the same shape as the array received in `__call__` method.\n",
    "\n",
    "        Returns:\n",
    "            An array of the same shape as `gradOutput`\"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "\n",
    "class ReLU(Activation):\n",
    "    \"\"\"Implements ReLU activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        self._x = x\n",
    "        self._input = x\n",
    "        return np.maximum(0, self._x)\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        return gradOutput * (self._x >= 0)\n",
    "\n",
    "\n",
    "class LeakyReLU(Activation):\n",
    "    \"\"\"Implements LeakyReLU activation layer\"\"\"\n",
    "\n",
    "    def __init__(self, slope: float = 0.03):\n",
    "        \"\"\"Initializes LeakyReLU layer.\n",
    "\n",
    "        Arguments:\n",
    "            slope: the slope coeffitient of the activation.\"\"\"\n",
    "        self._slope = slope\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        self._x = x\n",
    "        self._input = x\n",
    "        return np.where(x > 0, x, self._slope * x)\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        return np.where(self._x > 0, gradOutput, self._slope * gradOutput)\n",
    "\n",
    "class GeLU(Activation):\n",
    "    \"\"\"Implements GeLU activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        self._x = x\n",
    "        self._input = x\n",
    "        return self._x * norm.cdf(self._x)\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        return (self._x * norm.pdf(self._x) + norm.cdf(self._x)) * gradOutput\n",
    "\n",
    "\n",
    "class SiLU(Activation):\n",
    "    \"\"\"Implements SiLU (swish) activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        self._x = x\n",
    "        self._input = x\n",
    "        return self._x * self.sigmoid(self._x)\n",
    "\n",
    "    def sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        return (self._x * self.sigmoid(self._x) * (1 - self.sigmoid(self._x)) + self.sigmoid(self._x)) * gradOutput\n",
    "\n",
    "\n",
    "class Softplus(Activation):\n",
    "    \"\"\"Implements Softplus (SmoothReLU) activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        self._x = x\n",
    "        self._input = x\n",
    "        return np.log(1 + np.exp(self._x))\n",
    "\n",
    "    def sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        return self.sigmoid(self._x) * gradOutput\n",
    "\n",
    "\n",
    "class ELU(Activation):\n",
    "    \"\"\"Implements ELU activation layer\"\"\"\n",
    "\n",
    "    def __init__(self, alpha: float = 1):\n",
    "        \"\"\"Initializes ELU layer.\n",
    "\n",
    "        Arguments:\n",
    "            alpha: the alpha coeffitient of the activation.\"\"\"\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        self._x = x\n",
    "        self._input = x\n",
    "        return np.where(x > 0, x, self.alpha * (np.exp(x) - 1))\n",
    "        \n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        return np.where(self._x > 0, gradOutput, self.alpha * np.exp(self._x) * gradOutput)\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    \"\"\"Implements Sigmoid activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        self._x = x\n",
    "        self._input = x\n",
    "        return self.sigmoid(self._x)\n",
    "\n",
    "    def sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        return self.sigmoid(self._x) * (1 - self.sigmoid(self._x)) * gradOutput\n",
    "\n",
    "\n",
    "class Tanh(Activation):\n",
    "    \"\"\"Implements Tanh activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        self._x = x\n",
    "        self._input = x\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        return 1/(np.square(np.cosh(self._x))) * gradOutput\n",
    "\n",
    "\n",
    "class Softmax(Activation):\n",
    "    \"\"\"Implements Softmax activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Computes Softmax activation output\n",
    "        \n",
    "        Arguments:\n",
    "            x: Input array of shape (`batch_size`, `n_features`)\n",
    "\n",
    "        Returns:\n",
    "            An array of the same shape as `x`\"\"\"\n",
    "        self._x = x\n",
    "        self._input = x\n",
    "        out = np.exp(x)\n",
    "        \n",
    "        return out / np.sum(out, axis=1, keepdims=True)\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        \n",
    "        out = np.exp(self._x)\n",
    "        sum_out = np.sum(out, axis=1, keepdims=True)\n",
    "        sm = out / sum_out\n",
    "        \n",
    "        smismj = np.einsum('...i,...j->...ij', sm, sm)\n",
    "        b = np.eye(sm.shape[1])\n",
    "        smi = np.einsum('ij,jk->ijk', sm, b)\n",
    "        grad_ = smi - smismj\n",
    "        return np.einsum('...i,...ij->...j', gradOutput, grad_)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1,2,3,4,5], \n",
    "              [6,7,8,9,10],\n",
    "              [11,12,13,14,15]])\n",
    "grad_x = x * 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0321893 , -0.06215166, -0.10004289, -0.08464745,  0.2790313 ],\n",
       "       [-0.0321893 , -0.06215166, -0.10004289, -0.08464745,  0.2790313 ],\n",
       "       [-0.0321893 , -0.06215166, -0.10004289, -0.08464745,  0.2790313 ]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Softmax()\n",
    "a(x)\n",
    "a.grad(grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2, -1,  0,  1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ReLU()\n",
    "a(x)\n",
    "a.grad(grad_x)\n",
    "a.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24 , -0.216, -0.192, -0.168, -0.144, -0.12 , -0.096, -0.072,\n",
       "       -0.048, -0.024,  0.   ,  0.8  ,  1.6  ,  2.4  ,  3.2  ,  4.   ,\n",
       "        4.8  ,  5.6  ,  6.4  ,  7.2  ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = LeakyReLU()\n",
    "a(x)\n",
    "a.grad(grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.09472008e-21, 6.58003491e-17, 2.54694865e-13, 3.50914090e-10,\n",
       "       1.70249805e-07, 2.85877840e-05, 1.61167892e-03, 2.86695533e-02,\n",
       "       1.36370882e-01, 6.66523765e-02, 0.00000000e+00, 8.66652376e-01,\n",
       "       1.73637088e+00, 2.42866955e+00, 3.20161168e+00, 4.00002859e+00,\n",
       "       4.80000017e+00, 5.60000000e+00, 6.40000000e+00, 7.20000000e+00])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = GeLU()\n",
    "a(x)\n",
    "a.grad(grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.26848167e-03,  7.10654092e-03,  1.50179279e-02,  3.05787836e-02,\n",
       "        5.91668764e-02,  1.06189730e-01,  1.68526768e-01,  2.11449854e-01,\n",
       "        1.45254798e-01, -5.78635905e-02,  0.00000000e+00,  7.42136409e-01,\n",
       "        1.74525480e+00,  2.61144985e+00,  3.36852677e+00,  4.10618973e+00,\n",
       "        4.85916688e+00,  5.63057878e+00,  6.41501793e+00,  7.20710654e+00])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = SiLU()\n",
    "a(x)\n",
    "a.grad(grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.63199438e-04, -8.88550589e-04, -2.14696082e-03, -5.10653901e-03,\n",
       "       -1.18980104e-02, -2.69517880e-02, -5.86100444e-02, -1.19488964e-01,\n",
       "       -2.16536453e-01, -2.94303553e-01,  0.00000000e+00,  8.00000000e-01,\n",
       "        1.60000000e+00,  2.40000000e+00,  3.20000000e+00,  4.00000000e+00,\n",
       "        4.80000000e+00,  5.60000000e+00,  6.40000000e+00,  7.20000000e+00])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ELU()\n",
    "a(x)\n",
    "a.grad(grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.63182950e-04, -8.88440947e-04, -2.14624083e-03, -5.10188669e-03,\n",
       "       -1.18685912e-02, -2.67714037e-02, -5.75558719e-02, -1.13822096e-01,\n",
       "       -1.90724675e-01, -2.15153137e-01,  0.00000000e+00,  5.84846863e-01,\n",
       "        1.40927532e+00,  2.28617790e+00,  3.14244413e+00,  3.97322860e+00,\n",
       "        4.78813141e+00,  5.59489811e+00,  6.39785376e+00,  7.19911156e+00])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Softplus()\n",
    "a(x)\n",
    "a.grad(grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00036317, -0.00088833, -0.00214552, -0.00509724, -0.01183924,\n",
       "       -0.02659223, -0.05652066, -0.10842398, -0.16798974, -0.15728955,\n",
       "        0.        ,  0.15728955,  0.16798974,  0.10842398,  0.05652066,\n",
       "        0.02659223,  0.01183924,  0.00509724,  0.00214552,  0.00088833])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Sigmoid()\n",
    "a(x)\n",
    "a.grad(grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.59569156e-08, -4.38623403e-07, -2.88089982e-06, -1.86262123e-05,\n",
       "       -1.17967428e-04, -7.26332924e-04, -4.29104219e-03, -2.36784892e-02,\n",
       "       -1.13041320e-01, -3.35979473e-01,  0.00000000e+00,  3.35979473e-01,\n",
       "        1.13041320e-01,  2.36784892e-02,  4.29104219e-03,  7.26332924e-04,\n",
       "        1.17967428e-04,  1.86262123e-05,  2.88089982e-06,  4.38623403e-07])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Tanh()\n",
    "a(x)\n",
    "a.grad(grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 0],\n",
    "               [2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1,  0],\n",
       "        [ 0,  0]],\n",
       "\n",
       "       [[-4, -6],\n",
       "        [-6, -9]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.einsum('...i,...j->...ij', a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.eye(a.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[2., 0.],\n",
       "        [0., 3.]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.einsum('ij,jk->ijk', a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.],\n",
       "        [ 0.,  0.]],\n",
       "\n",
       "       [[-2., -6.],\n",
       "        [-6., -6.]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = np.einsum('ij,jk->ijk', a, b) - np.einsum('...i,...j->...ij', a, a)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.],\n",
       "       [-22., -30.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.einsum('...i,...ij->...j', a, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
