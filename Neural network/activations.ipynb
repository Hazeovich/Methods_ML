{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "class Activation:\n",
    "    \"\"\"Base activation class\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._input = None\n",
    "\n",
    "    @property\n",
    "    def input(self):\n",
    "        \"\"\"Returns the last input received by the activation\"\"\"\n",
    "        return self._input\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Computes activation output\n",
    "        \n",
    "        Arguments:\n",
    "            x: Input array of shape (`batch_size`, ...)\n",
    "\n",
    "        Returns:\n",
    "            An array of the same shape as `x`\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Computes loss gradient with respect to the activation input.\n",
    "        \n",
    "        Arguments:\n",
    "            gradOutput: Gradient of loss function with recpect to the activation output.\n",
    "                An array of the same shape as the array received in `__call__` method.\n",
    "\n",
    "        Returns:\n",
    "            An array of the same shape as `gradOutput`\"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "\n",
    "class ReLU(Activation):\n",
    "    \"\"\"Implements ReLU activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        self._x = x\n",
    "        self._input = x\n",
    "        return np.maximum(0, self._x)\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        return gradOutput * (self._x >= 0)\n",
    "\n",
    "\n",
    "class LeakyReLU(Activation):\n",
    "    \"\"\"Implements LeakyReLU activation layer\"\"\"\n",
    "\n",
    "    def __init__(self, slope: float = 0.03):\n",
    "        \"\"\"Initializes LeakyReLU layer.\n",
    "\n",
    "        Arguments:\n",
    "            slope: the slope coeffitient of the activation.\"\"\"\n",
    "        self._slope = slope\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        self._x = x\n",
    "        self._input = x\n",
    "        return np.where(x > 0, x, self._slope * x)\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        return np.where(self._x > 0, gradOutput, self._slope * gradOutput)\n",
    "\n",
    "class GeLU(Activation):\n",
    "    \"\"\"Implements GeLU activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        self._x = x\n",
    "        self._input = x\n",
    "        return self._x * norm.cdf(self._x)\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        return (self._x * norm.pdf(self._x) + norm.cdf(self._x)) * gradOutput\n",
    "\n",
    "\n",
    "class SiLU(Activation):\n",
    "    \"\"\"Implements SiLU (swish) activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        self._x = x\n",
    "        self._input = x\n",
    "        return self._x * self.sigmoid(self._x)\n",
    "\n",
    "    def sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        return (self._x * self.sigmoid(self._x) * (1 - self.sigmoid(self._x)) + self.sigmoid(self._x)) * gradOutput\n",
    "\n",
    "\n",
    "class Softplus(Activation):\n",
    "    \"\"\"Implements Softplus (SmoothReLU) activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        self._x = x\n",
    "        self._input = x\n",
    "        return np.log(1 + np.exp(self._x))\n",
    "\n",
    "    def sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        return self.sigmoid(self._x) * gradOutput\n",
    "\n",
    "\n",
    "class ELU(Activation):\n",
    "    \"\"\"Implements ELU activation layer\"\"\"\n",
    "\n",
    "    def __init__(self, alpha: float = 1):\n",
    "        \"\"\"Initializes ELU layer.\n",
    "\n",
    "        Arguments:\n",
    "            alpha: the alpha coeffitient of the activation.\"\"\"\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        self._x = x\n",
    "        self._input = x\n",
    "        return np.where(x > 0, x, self.alpha * (np.exp(x) - 1))\n",
    "        \n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        return np.where(self._x > 0, gradOutput, self.alpha * np.exp(self._x) * gradOutput)\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    \"\"\"Implements Sigmoid activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        self._x = x\n",
    "        self._input = x\n",
    "        return self.sigmoid(self._x)\n",
    "\n",
    "    def sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        return self.sigmoid(self._x) * (1 - self.sigmoid(self._x)) * gradOutput\n",
    "\n",
    "\n",
    "class Tanh(Activation):\n",
    "    \"\"\"Implements Tanh activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        self._x = x\n",
    "        self._input = x\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        return 1/(np.square(np.cosh(self._x))) * gradOutput\n",
    "\n",
    "\n",
    "class Softmax(Activation):\n",
    "    \"\"\"Implements Softmax activation layer\"\"\"\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Computes Softmax activation output\n",
    "        \n",
    "        Arguments:\n",
    "            x: Input array of shape (`batch_size`, `n_features`)\n",
    "\n",
    "        Returns:\n",
    "            An array of the same shape as `x`\"\"\"\n",
    "        self._x = x\n",
    "        self._input = x\n",
    "        out = np.exp(x)\n",
    "        \n",
    "        return out / np.sum(out, axis=1, keepdims=True)\n",
    "\n",
    "    def grad(self, gradOutput: np.ndarray) -> np.ndarray:\n",
    "        \n",
    "        out = np.exp(self._x)\n",
    "        sum_out = np.sum(out, axis=1, keepdims=True)\n",
    "        sm = out / sum_out\n",
    "        \n",
    "        smismj = np.einsum('...i,...j->...ij', sm, sm)\n",
    "        b = np.eye(sm.shape[1])\n",
    "        smi = np.einsum('ij,jk->ijk', sm, b)\n",
    "        grad_ = smi - smismj\n",
    "        return np.einsum('...i,...ij->...j', gradOutput, grad_)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1,2,3,4,5], \n",
    "              [6,7,8,9,10],\n",
    "              [11,12,13,14,15]])\n",
    "grad_x = x * 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0321893 , -0.06215166, -0.10004289, -0.08464745,  0.2790313 ],\n",
       "       [-0.0321893 , -0.06215166, -0.10004289, -0.08464745,  0.2790313 ],\n",
       "       [-0.0321893 , -0.06215166, -0.10004289, -0.08464745,  0.2790313 ]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Softmax()\n",
    "a(x)\n",
    "a.grad(grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5],\n",
       "       [ 6,  7,  8,  9, 10],\n",
       "       [11, 12, 13, 14, 15]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ReLU()\n",
    "a(x)\n",
    "a.grad(grad_x)\n",
    "a.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.8,  1.6,  2.4,  3.2,  4. ],\n",
       "       [ 4.8,  5.6,  6.4,  7.2,  8. ],\n",
       "       [ 8.8,  9.6, 10.4, 11.2, 12. ]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = LeakyReLU()\n",
    "a(x)\n",
    "a.grad(grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.86665238,  1.73637088,  2.42866955,  3.20161168,  4.00002859],\n",
       "       [ 4.80000017,  5.6       ,  6.4       ,  7.2       ,  8.        ],\n",
       "       [ 8.8       ,  9.6       , 10.4       , 11.2       , 12.        ]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = GeLU()\n",
    "a(x)\n",
    "a.grad(grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.74213641,  1.7452548 ,  2.61144985,  3.36852677,  4.10618973],\n",
       "       [ 4.85916688,  5.63057878,  6.41501793,  7.20710654,  8.00326848],\n",
       "       [ 8.8014697 ,  9.60064882, 10.40028209, 11.20012107, 12.00005139]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = SiLU()\n",
    "a(x)\n",
    "a.grad(grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.8,  1.6,  2.4,  3.2,  4. ],\n",
       "       [ 4.8,  5.6,  6.4,  7.2,  8. ],\n",
       "       [ 8.8,  9.6, 10.4, 11.2, 12. ]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ELU()\n",
    "a(x)\n",
    "a.grad(grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.58484686,  1.40927532,  2.2861779 ,  3.14244413,  3.9732286 ],\n",
       "       [ 4.78813141,  5.59489811,  6.39785376,  7.19911156,  7.99963682],\n",
       "       [ 8.79985303,  9.59994102, 10.39997649, 11.19999069, 11.99999633]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Softplus()\n",
    "a(x)\n",
    "a.grad(grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.57289547e-01, 1.67989737e-01, 1.08423983e-01, 5.65206599e-02,\n",
       "        2.65922267e-02],\n",
       "       [1.18392446e-02, 5.09723861e-03, 2.14552109e-03, 8.88331318e-04,\n",
       "        3.63166462e-04],\n",
       "       [1.46970058e-04, 5.89837138e-05, 2.35073196e-05, 9.31310617e-06,\n",
       "        3.67082560e-06]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Sigmoid()\n",
    "a(x)\n",
    "a.grad(grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.35979473e-01, 1.13041320e-01, 2.36784892e-02, 4.29104219e-03,\n",
       "        7.26332924e-04],\n",
       "       [1.17967428e-04, 1.86262123e-05, 2.88089982e-06, 4.38623403e-07,\n",
       "        6.59569156e-08],\n",
       "       [9.81892768e-09, 1.44965166e-09, 2.12538104e-10, 3.09765125e-11,\n",
       "        4.49165903e-12]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Tanh()\n",
    "a(x)\n",
    "a.grad(grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 0],\n",
    "               [2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1,  0],\n",
       "        [ 0,  0]],\n",
       "\n",
       "       [[-4, -6],\n",
       "        [-6, -9]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.einsum('...i,...j->...ij', a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.eye(a.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[2., 0.],\n",
       "        [0., 3.]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.einsum('ij,jk->ijk', a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.],\n",
       "        [ 0.,  0.]],\n",
       "\n",
       "       [[-2., -6.],\n",
       "        [-6., -6.]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = np.einsum('ij,jk->ijk', a, b) - np.einsum('...i,...j->...ij', a, a)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.],\n",
       "       [-22., -30.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.einsum('...i,...ij->...j', a, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
